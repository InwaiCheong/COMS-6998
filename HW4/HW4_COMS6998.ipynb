{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW4_COMS6998.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "environment": {
      "name": "tf2-2-3-gpu.2-3.m59",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YpPrZIc1jX0"
      },
      "source": [
        "# In Wai Cheong (ic2518)\n",
        "# COMS 6998 HW 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCigOFJPHe7L"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twT3u-HF1nNk"
      },
      "source": [
        "## Question 1) \n",
        "\n",
        "For fine-tuning you will select a target dataset from the Visual-Decathlon challenge. Their web site (link below) has several datasets which you can download. Select any one of the visual decathlon dataset\n",
        "and make it your target dataset for transfer learning. Important : Do not select Imagenet1K as\n",
        "the target dataset. <br>\n",
        "\n",
        "(1a) Finetuning: You will first load a pretrained model (Resnet50) and change the final fully connected\n",
        "layer output to the number of classes in the target dataset. Describe your target dataset features,\n",
        "number of classes and distribution of images per class (i.e., number of images per class). Show\n",
        "any 4 sample images (belonging to 2 different classes) from your target dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "HW3zPoFmq5Jq",
        "outputId": "be0a83b6-103f-46ee-9d10-bb0a79a22331"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar100.load_data()\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train  /= 255.0\n",
        "X_test /= 255.0\n",
        "\n",
        "print('Number of samples in training set:',len(X_train))\n",
        "print('Number of classes:',len(np.unique(X_test)))\n",
        "print('100 classes containing 60 images each')\n",
        "\n",
        "fig = plt.figure(figsize = (8,10))\n",
        "plt.subplot(1,4,1)\n",
        "plt.imshow(X_train[0])\n",
        "plt.subplot(1,4,2)\n",
        "plt.imshow(X_train[1])\n",
        "plt.subplot(1,4,3)\n",
        "plt.imshow(X_train[2])\n",
        "plt.subplot(1,4,4)\n",
        "plt.imshow(X_train[3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 11s 0us/step\n",
            "Number of samples in training set: 50000\n",
            "Number of classes: 256\n",
            "100 classes containing 60 images each\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fef21b9f490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x720 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_khz6mKgHAZO",
        "outputId": "cb96b876-45f3-44af-ec6f-83347fa1913a"
      },
      "source": [
        "from keras.applications.resnet50 import ResNet50\n",
        "# load model\n",
        "model = ResNet50(input_shape = (32,32,3),weights=None,classes=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn5PzUgoGgMp",
        "outputId": "219c7797-9af7-41d8-b1dc-38a682eb5c44"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n",
            "(50000, 100)\n",
            "(10000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dHBxvEU13SU"
      },
      "source": [
        "(1b) First finetune by setting the same value of hyperparameters (learning rate=0.001, momentum=0.9)\n",
        "for all the layers. Keep batch size of 64 and train for 200-300 epochs or until model converges well.\n",
        "You will use a multi-step learning rate schedule and decay by a factor of 0.1 (\n",
        " = 0:1 in the link\n",
        "below). You can choose steps at which you want to decay the learning rate but do 3 drops during\n",
        "the training. So the first drop will bring down the learning rate to 0.0001, second to 0.00001, third\n",
        "to 0.000001. For example, if training for 200 epochs, first drop can happen at epoch 60, second at\n",
        "epoch 120 and third at epoch 180."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "NHTZmiliD87o",
        "outputId": "0cab266a-0eb2-4914-cb1b-176c1b8494a7"
      },
      "source": [
        "#SGD Hyperparameters\n",
        "tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=False, name=\"SGD\")\n",
        "\n",
        "#Compiling Model\n",
        "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#Fitting model\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_test,y_test),shuffle=True,verbose=0)\n",
        "history.history['accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.08077999949455261,\n",
              " 0.1469999998807907,\n",
              " 0.19043999910354614,\n",
              " 0.22519999742507935,\n",
              " 0.2630000114440918,\n",
              " 0.303600013256073,\n",
              " 0.33823999762535095,\n",
              " 0.3697800040245056,\n",
              " 0.40803998708724976,\n",
              " 0.43935999274253845,\n",
              " 0.4765799939632416,\n",
              " 0.5186200141906738,\n",
              " 0.5549399852752686,\n",
              " 0.5952000021934509,\n",
              " 0.6313199996948242,\n",
              " 0.6670399904251099,\n",
              " 0.6957399845123291,\n",
              " 0.7367200255393982,\n",
              " 0.7719200253486633,\n",
              " 0.7865599989891052,\n",
              " 0.8120200037956238,\n",
              " 0.8421199917793274,\n",
              " 0.8501200079917908,\n",
              " 0.8646799921989441,\n",
              " 0.8796799778938293,\n",
              " 0.890500009059906,\n",
              " 0.9078400135040283,\n",
              " 0.9072399735450745,\n",
              " 0.9130399823188782,\n",
              " 0.9183400273323059,\n",
              " 0.9259799718856812,\n",
              " 0.9387000203132629,\n",
              " 0.942579984664917,\n",
              " 0.9464799761772156,\n",
              " 0.9434400200843811,\n",
              " 0.9456999897956848,\n",
              " 0.9497799873352051,\n",
              " 0.9567199945449829,\n",
              " 0.9549199938774109,\n",
              " 0.9656000137329102,\n",
              " 0.9661200046539307,\n",
              " 0.9722800254821777,\n",
              " 0.9744799733161926,\n",
              " 0.9783999919891357,\n",
              " 0.9786199927330017,\n",
              " 0.9808599948883057,\n",
              " 0.9831600189208984,\n",
              " 0.9806200265884399,\n",
              " 0.9749400019645691,\n",
              " 0.9807999730110168,\n",
              " 0.9792199730873108,\n",
              " 0.9884799718856812,\n",
              " 0.9910799860954285,\n",
              " 0.9924200177192688,\n",
              " 0.9922199845314026,\n",
              " 0.992139995098114,\n",
              " 0.9928799867630005,\n",
              " 0.9947800040245056,\n",
              " 0.9929599761962891,\n",
              " 0.9936000108718872,\n",
              " 0.9930599927902222,\n",
              " 0.9945799708366394,\n",
              " 0.9941800236701965,\n",
              " 0.994920015335083,\n",
              " 0.9961199760437012,\n",
              " 0.9968799948692322,\n",
              " 0.9968799948692322,\n",
              " 0.9951199889183044,\n",
              " 0.9961199760437012,\n",
              " 0.9958000183105469,\n",
              " 0.9953799843788147,\n",
              " 0.9957000017166138,\n",
              " 0.9940800070762634,\n",
              " 0.9946200251579285,\n",
              " 0.9933599829673767,\n",
              " 0.9938600063323975,\n",
              " 0.9965400099754333,\n",
              " 0.9966599941253662,\n",
              " 0.9946399927139282,\n",
              " 0.9959400296211243,\n",
              " 0.9962000250816345,\n",
              " 0.9929199814796448,\n",
              " 0.9958599805831909,\n",
              " 0.9967799782752991,\n",
              " 0.9961199760437012,\n",
              " 0.9934599995613098,\n",
              " 0.9963399767875671,\n",
              " 0.9964399933815002,\n",
              " 0.99617999792099,\n",
              " 0.9969199895858765,\n",
              " 0.9968199729919434,\n",
              " 0.9973199963569641,\n",
              " 0.9977399706840515,\n",
              " 0.9922199845314026,\n",
              " 0.9944800138473511,\n",
              " 0.9959999918937683,\n",
              " 0.9965199828147888,\n",
              " 0.9965999722480774,\n",
              " 0.9959200024604797,\n",
              " 0.9944000244140625,\n",
              " 0.9962199926376343,\n",
              " 0.9964200258255005,\n",
              " 0.9962000250816345,\n",
              " 0.9950000047683716,\n",
              " 0.995419979095459,\n",
              " 0.996999979019165,\n",
              " 0.9968600273132324,\n",
              " 0.997759997844696,\n",
              " 0.9979000091552734,\n",
              " 0.9969000220298767,\n",
              " 0.9982799887657166,\n",
              " 0.9981200098991394,\n",
              " 0.9967600107192993,\n",
              " 0.996999979019165,\n",
              " 0.9966199994087219,\n",
              " 0.996940016746521,\n",
              " 0.9971799850463867,\n",
              " 0.9979000091552734,\n",
              " 0.9982200264930725,\n",
              " 0.9981200098991394,\n",
              " 0.9983999729156494,\n",
              " 0.9970399737358093,\n",
              " 0.9974200129508972,\n",
              " 0.9961599707603455,\n",
              " 0.9940599799156189,\n",
              " 0.9960200190544128,\n",
              " 0.9972000122070312,\n",
              " 0.9970600008964539,\n",
              " 0.9979000091552734,\n",
              " 0.9980999827384949,\n",
              " 0.9986799955368042,\n",
              " 0.9983199834823608,\n",
              " 0.997979998588562,\n",
              " 0.9977800250053406,\n",
              " 0.9965599775314331,\n",
              " 0.9963200092315674,\n",
              " 0.9947800040245056,\n",
              " 0.9967600107192993,\n",
              " 0.9975000023841858,\n",
              " 0.9977200031280518,\n",
              " 0.9988200068473816,\n",
              " 0.997439980506897,\n",
              " 0.9972000122070312,\n",
              " 0.9977200031280518,\n",
              " 0.9977200031280518,\n",
              " 0.9978200197219849,\n",
              " 0.9983400106430054,\n",
              " 0.9982799887657166,\n",
              " 0.9982200264930725,\n",
              " 0.9983199834823608,\n",
              " 0.9979599714279175,\n",
              " 0.9985600113868713,\n",
              " 0.9981600046157837,\n",
              " 0.9984599947929382,\n",
              " 0.998960018157959,\n",
              " 0.9988200068473816,\n",
              " 0.9985600113868713,\n",
              " 0.9984400272369385,\n",
              " 0.9969599843025208,\n",
              " 0.9968600273132324,\n",
              " 0.9982200264930725,\n",
              " 0.997439980506897,\n",
              " 0.9973599910736084,\n",
              " 0.9962400197982788,\n",
              " 0.9957200288772583,\n",
              " 0.997160017490387,\n",
              " 0.9953600168228149,\n",
              " 0.9956799745559692,\n",
              " 0.9958800077438354,\n",
              " 0.9970200061798096,\n",
              " 0.9965599775314331,\n",
              " 0.9964399933815002,\n",
              " 0.9969199895858765,\n",
              " 0.9962999820709229,\n",
              " 0.9968799948692322,\n",
              " 0.9978799819946289,\n",
              " 0.998199999332428,\n",
              " 0.9984999895095825,\n",
              " 0.9988800287246704,\n",
              " 0.9988600015640259,\n",
              " 0.9975799918174744,\n",
              " 0.9979199767112732,\n",
              " 0.9966800212860107,\n",
              " 0.9976599812507629,\n",
              " 0.9980800151824951,\n",
              " 0.9984599947929382,\n",
              " 0.9963200092315674,\n",
              " 0.9970999956130981,\n",
              " 0.9970399737358093,\n",
              " 0.9968199729919434,\n",
              " 0.9976999759674072,\n",
              " 0.9966599941253662,\n",
              " 0.994920015335083,\n",
              " 0.9973999857902527,\n",
              " 0.9955999851226807,\n",
              " 0.9974799752235413,\n",
              " 0.9973400235176086,\n",
              " 0.9973599910736084,\n",
              " 0.9969599843025208,\n",
              " 0.9965199828147888]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTMEyCiK13a6"
      },
      "source": [
        "(1c) Next keeping all the hyperparameters same as before, change the learning rate to 0.01 and 0.1\n",
        "uniformly for all the layers. This means keep all the layers at same learning rate. So you will be\n",
        "doing two experiments, one keeping learning rate of all layers at 0.01 and one with 0.1. Again\n",
        "finetune the model and report the final accuracy. How does the accuracy with the three learning\n",
        "rates compare ? Which learning rate gives you the best accuracy on the target dataset ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yth4Xt2KJ8W6",
        "outputId": "942f3137-b359-4f0a-89eb-a267b0de0e26"
      },
      "source": [
        "#SGD Hyperparameters\n",
        "tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=False, name=\"SGD\")\n",
        "\n",
        "#Compiling Model\n",
        "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#Fitting model\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_test,y_test),shuffle=True,verbose=0)\n",
        "history.history['accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9964200258255005,\n",
              " 0.9967399835586548,\n",
              " 0.9954400062561035,\n",
              " 0.9970399737358093,\n",
              " 0.9980800151824951,\n",
              " 0.9975200295448303,\n",
              " 0.9982799887657166,\n",
              " 0.9985399842262268,\n",
              " 0.9974799752235413,\n",
              " 0.9979000091552734,\n",
              " 0.9980000257492065,\n",
              " 0.9983400106430054,\n",
              " 0.9984400272369385,\n",
              " 0.9985600113868713,\n",
              " 0.9975000023841858,\n",
              " 0.9976400136947632,\n",
              " 0.9978600144386292,\n",
              " 0.997759997844696,\n",
              " 0.9959800243377686,\n",
              " 0.9972000122070312,\n",
              " 0.9975600242614746,\n",
              " 0.997979998588562,\n",
              " 0.9975799918174744,\n",
              " 0.9976400136947632,\n",
              " 0.9977800250053406,\n",
              " 0.998520016670227,\n",
              " 0.9973999857902527,\n",
              " 0.9969800114631653,\n",
              " 0.9979199767112732,\n",
              " 0.9982799887657166,\n",
              " 0.9980199933052063,\n",
              " 0.9985600113868713,\n",
              " 0.9983999729156494,\n",
              " 0.9988800287246704,\n",
              " 0.998199999332428,\n",
              " 0.9979599714279175,\n",
              " 0.9976599812507629,\n",
              " 0.9973199963569641,\n",
              " 0.9973999857902527,\n",
              " 0.9978399872779846,\n",
              " 0.9986199736595154,\n",
              " 0.9980599880218506,\n",
              " 0.9984599947929382,\n",
              " 0.9988200068473816,\n",
              " 0.9988399744033813,\n",
              " 0.9975799918174744,\n",
              " 0.9983599781990051,\n",
              " 0.9985600113868713,\n",
              " 0.9979599714279175,\n",
              " 0.9981600046157837,\n",
              " 0.9986799955368042,\n",
              " 0.9980999827384949,\n",
              " 0.9987599849700928,\n",
              " 0.9987800121307373,\n",
              " 0.9976800084114075,\n",
              " 0.996999979019165,\n",
              " 0.9973000288009644,\n",
              " 0.9972000122070312,\n",
              " 0.9983400106430054,\n",
              " 0.9969199895858765,\n",
              " 0.9977800250053406,\n",
              " 0.9967600107192993,\n",
              " 0.996720016002655,\n",
              " 0.9965800046920776,\n",
              " 0.9975000023841858,\n",
              " 0.9978399872779846,\n",
              " 0.9973999857902527,\n",
              " 0.9982600212097168,\n",
              " 0.9973400235176086,\n",
              " 0.9959999918937683,\n",
              " 0.997979998588562,\n",
              " 0.9981600046157837,\n",
              " 0.9975399971008301,\n",
              " 0.9977399706840515,\n",
              " 0.998740017414093,\n",
              " 0.9985399842262268,\n",
              " 0.9984999895095825,\n",
              " 0.9986199736595154,\n",
              " 0.9988399744033813,\n",
              " 0.9987999796867371,\n",
              " 0.9971200227737427,\n",
              " 0.9976800084114075,\n",
              " 0.9975399971008301,\n",
              " 0.9965599775314331,\n",
              " 0.9973199963569641,\n",
              " 0.9983999729156494,\n",
              " 0.9973999857902527,\n",
              " 0.9970200061798096,\n",
              " 0.9979400038719177,\n",
              " 0.9978399872779846,\n",
              " 0.9986000061035156,\n",
              " 0.9984800219535828,\n",
              " 0.9989399909973145,\n",
              " 0.9986799955368042,\n",
              " 0.9982799887657166,\n",
              " 0.9982399940490723,\n",
              " 0.9978200197219849,\n",
              " 0.998420000076294,\n",
              " 0.9986199736595154,\n",
              " 0.9988399744033813,\n",
              " 0.9987599849700928,\n",
              " 0.9979400038719177,\n",
              " 0.9988200068473816,\n",
              " 0.9983199834823608,\n",
              " 0.9974799752235413,\n",
              " 0.9983800053596497,\n",
              " 0.9988200068473816,\n",
              " 0.9988800287246704,\n",
              " 0.999019980430603,\n",
              " 0.9980999827384949,\n",
              " 0.9982399940490723,\n",
              " 0.9988800287246704,\n",
              " 0.9990400075912476,\n",
              " 0.9990800023078918,\n",
              " 0.9987199902534485,\n",
              " 0.9987000226974487,\n",
              " 0.9987999796867371,\n",
              " 0.9978600144386292,\n",
              " 0.9988600015640259,\n",
              " 0.9982399940490723,\n",
              " 0.9985399842262268,\n",
              " 0.9980000257492065,\n",
              " 0.9983800053596497,\n",
              " 0.9984999895095825,\n",
              " 0.9983000159263611,\n",
              " 0.9985799789428711,\n",
              " 0.9986400008201599,\n",
              " 0.9987999796867371,\n",
              " 0.9977399706840515,\n",
              " 0.9988600015640259,\n",
              " 0.9989799857139587,\n",
              " 0.9984999895095825,\n",
              " 0.9988600015640259,\n",
              " 0.9986199736595154,\n",
              " 0.9987199902534485,\n",
              " 0.9982600212097168,\n",
              " 0.9991400241851807,\n",
              " 0.9985399842262268,\n",
              " 0.9987199902534485,\n",
              " 0.9988800287246704,\n",
              " 0.9976800084114075,\n",
              " 0.9984400272369385,\n",
              " 0.9985799789428711,\n",
              " 0.9986400008201599,\n",
              " 0.9981399774551392,\n",
              " 0.9981600046157837,\n",
              " 0.9979400038719177,\n",
              " 0.9977200031280518,\n",
              " 0.9970399737358093,\n",
              " 0.9983199834823608,\n",
              " 0.9983400106430054,\n",
              " 0.998420000076294,\n",
              " 0.9977399706840515,\n",
              " 0.9968400001525879,\n",
              " 0.9948199987411499,\n",
              " 0.9973800182342529,\n",
              " 0.9977200031280518,\n",
              " 0.9944400191307068,\n",
              " 0.9973999857902527,\n",
              " 0.9978399872779846,\n",
              " 0.9985399842262268,\n",
              " 0.9984800219535828,\n",
              " 0.9983000159263611,\n",
              " 0.9990000128746033,\n",
              " 0.9978399872779846,\n",
              " 0.9981600046157837,\n",
              " 0.9987599849700928,\n",
              " 0.9988800287246704,\n",
              " 0.9986600279808044,\n",
              " 0.9990599751472473,\n",
              " 0.9993399977684021,\n",
              " 0.9991199970245361,\n",
              " 0.9990000128746033,\n",
              " 0.9981200098991394,\n",
              " 0.9978600144386292,\n",
              " 0.9989200234413147,\n",
              " 0.9988800287246704,\n",
              " 0.9988999962806702,\n",
              " 0.9987800121307373,\n",
              " 0.9983599781990051,\n",
              " 0.9987000226974487,\n",
              " 0.9989399909973145,\n",
              " 0.9989399909973145,\n",
              " 0.9984999895095825,\n",
              " 0.9990599751472473,\n",
              " 0.9985799789428711,\n",
              " 0.9988800287246704,\n",
              " 0.9990400075912476,\n",
              " 0.9990800023078918,\n",
              " 0.999239981174469,\n",
              " 0.9991000294685364,\n",
              " 0.9990400075912476,\n",
              " 0.9992799758911133,\n",
              " 0.9987800121307373,\n",
              " 0.9984800219535828,\n",
              " 0.9985799789428711,\n",
              " 0.9983999729156494,\n",
              " 0.9989799857139587,\n",
              " 0.9989399909973145,\n",
              " 0.999180018901825]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D20MuLC_J8c8",
        "outputId": "9ae0387e-3b27-4740-964a-7cb1a2db5a81"
      },
      "source": [
        "#SGD Hyperparameters\n",
        "tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name=\"SGD\")\n",
        "\n",
        "#Compiling Model\n",
        "model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "#Fitting model\n",
        "history = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_test,y_test),shuffle=True,verbose=0)\n",
        "history.history['accuracy']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9988200068473816,\n",
              " 0.9990599751472473,\n",
              " 0.9983400106430054,\n",
              " 0.9986799955368042,\n",
              " 0.9989200234413147,\n",
              " 0.9990599751472473,\n",
              " 0.999180018901825,\n",
              " 0.999019980430603,\n",
              " 0.9991999864578247,\n",
              " 0.998740017414093,\n",
              " 0.9991599917411804,\n",
              " 0.9993000030517578,\n",
              " 0.999239981174469,\n",
              " 0.9992799758911133,\n",
              " 0.9991599917411804,\n",
              " 0.9993199706077576,\n",
              " 0.999180018901825,\n",
              " 0.9988600015640259,\n",
              " 0.9990400075912476,\n",
              " 0.9986000061035156,\n",
              " 0.999180018901825,\n",
              " 0.9989799857139587,\n",
              " 0.998960018157959,\n",
              " 0.9990800023078918,\n",
              " 0.9983999729156494,\n",
              " 0.9986000061035156,\n",
              " 0.9988200068473816,\n",
              " 0.9982399940490723,\n",
              " 0.998420000076294,\n",
              " 0.9983199834823608,\n",
              " 0.998740017414093,\n",
              " 0.9988800287246704,\n",
              " 0.9986400008201599,\n",
              " 0.9977999925613403,\n",
              " 0.9976199865341187,\n",
              " 0.9975399971008301,\n",
              " 0.9983599781990051,\n",
              " 0.998740017414093,\n",
              " 0.9983400106430054,\n",
              " 0.9986000061035156,\n",
              " 0.9988399744033813,\n",
              " 0.9992799758911133,\n",
              " 0.9993000030517578,\n",
              " 0.9984999895095825,\n",
              " 0.9988800287246704,\n",
              " 0.997979998588562,\n",
              " 0.9987800121307373,\n",
              " 0.9968199729919434,\n",
              " 0.9972800016403198,\n",
              " 0.996999979019165,\n",
              " 0.9976400136947632,\n",
              " 0.9976199865341187,\n",
              " 0.9987599849700928,\n",
              " 0.9966599941253662,\n",
              " 0.9965199828147888,\n",
              " 0.9976000189781189,\n",
              " 0.9968199729919434,\n",
              " 0.9976400136947632,\n",
              " 0.9978399872779846,\n",
              " 0.9983000159263611,\n",
              " 0.9973400235176086,\n",
              " 0.998420000076294,\n",
              " 0.998740017414093,\n",
              " 0.9974600076675415,\n",
              " 0.998420000076294,\n",
              " 0.9984599947929382,\n",
              " 0.9987800121307373,\n",
              " 0.9988600015640259,\n",
              " 0.9977399706840515,\n",
              " 0.9987999796867371,\n",
              " 0.9991000294685364,\n",
              " 0.9987199902534485,\n",
              " 0.9990000128746033,\n",
              " 0.997160017490387,\n",
              " 0.9972400069236755,\n",
              " 0.9966199994087219,\n",
              " 0.99617999792099,\n",
              " 0.9968400001525879,\n",
              " 0.9973000288009644,\n",
              " 0.998740017414093,\n",
              " 0.9982799887657166,\n",
              " 0.9985399842262268,\n",
              " 0.9986000061035156,\n",
              " 0.9986400008201599,\n",
              " 0.9982399940490723,\n",
              " 0.9985600113868713,\n",
              " 0.9987000226974487,\n",
              " 0.9990400075912476,\n",
              " 0.9990400075912476,\n",
              " 0.9983000159263611,\n",
              " 0.9980000257492065,\n",
              " 0.9981799721717834,\n",
              " 0.9983400106430054,\n",
              " 0.9988600015640259,\n",
              " 0.9987000226974487,\n",
              " 0.998740017414093,\n",
              " 0.997759997844696,\n",
              " 0.9978799819946289,\n",
              " 0.9976999759674072,\n",
              " 0.9981799721717834,\n",
              " 0.9975399971008301,\n",
              " 0.9982600212097168,\n",
              " 0.9983199834823608,\n",
              " 0.9985600113868713,\n",
              " 0.9985600113868713,\n",
              " 0.9989799857139587,\n",
              " 0.9990400075912476,\n",
              " 0.9984800219535828,\n",
              " 0.9983599781990051,\n",
              " 0.9987000226974487,\n",
              " 0.9987199902534485,\n",
              " 0.9986799955368042,\n",
              " 0.998520016670227,\n",
              " 0.9991400241851807,\n",
              " 0.999180018901825,\n",
              " 0.9990400075912476,\n",
              " 0.9983599781990051,\n",
              " 0.9990000128746033,\n",
              " 0.9990800023078918,\n",
              " 0.9993000030517578,\n",
              " 0.9988999962806702,\n",
              " 0.9986799955368042,\n",
              " 0.998420000076294,\n",
              " 0.9988600015640259,\n",
              " 0.9990599751472473,\n",
              " 0.9990400075912476,\n",
              " 0.9993799924850464,\n",
              " 0.9989799857139587,\n",
              " 0.9992600083351135,\n",
              " 0.9991000294685364,\n",
              " 0.9988999962806702,\n",
              " 0.9990000128746033,\n",
              " 0.9990000128746033,\n",
              " 0.9991599917411804,\n",
              " 0.9990599751472473,\n",
              " 0.9987000226974487,\n",
              " 0.9963799715042114,\n",
              " 0.9984599947929382,\n",
              " 0.998740017414093,\n",
              " 0.999019980430603,\n",
              " 0.9985799789428711,\n",
              " 0.9986000061035156,\n",
              " 0.9988999962806702,\n",
              " 0.9990000128746033,\n",
              " 0.9985399842262268,\n",
              " 0.9986000061035156,\n",
              " 0.9988999962806702,\n",
              " 0.9987999796867371,\n",
              " 0.9985600113868713,\n",
              " 0.9990599751472473,\n",
              " 0.9990599751472473,\n",
              " 0.9987599849700928,\n",
              " 0.9990599751472473,\n",
              " 0.9987800121307373,\n",
              " 0.9984400272369385,\n",
              " 0.9985600113868713,\n",
              " 0.9987599849700928,\n",
              " 0.9990800023078918,\n",
              " 0.9983199834823608,\n",
              " 0.9981600046157837,\n",
              " 0.9988399744033813,\n",
              " 0.9982399940490723,\n",
              " 0.9982399940490723,\n",
              " 0.9987999796867371,\n",
              " 0.999180018901825,\n",
              " 0.9991000294685364,\n",
              " 0.9991999864578247,\n",
              " 0.9985399842262268,\n",
              " 0.9987199902534485,\n",
              " 0.9979400038719177,\n",
              " 0.9985399842262268,\n",
              " 0.9988399744033813,\n",
              " 0.9992200136184692,\n",
              " 0.9990800023078918,\n",
              " 0.9986600279808044,\n",
              " 0.9986199736595154,\n",
              " 0.9983999729156494,\n",
              " 0.9981799721717834,\n",
              " 0.9991000294685364,\n",
              " 0.9986600279808044,\n",
              " 0.9977999925613403,\n",
              " 0.9990599751472473,\n",
              " 0.9990599751472473,\n",
              " 0.999019980430603,\n",
              " 0.9993799924850464,\n",
              " 0.9991199970245361,\n",
              " 0.9990400075912476,\n",
              " 0.9990800023078918,\n",
              " 0.9993199706077576,\n",
              " 0.9984999895095825,\n",
              " 0.9991599917411804,\n",
              " 0.999019980430603,\n",
              " 0.9986199736595154,\n",
              " 0.9981799721717834,\n",
              " 0.9978799819946289,\n",
              " 0.9984599947929382,\n",
              " 0.9980999827384949,\n",
              " 0.9986199736595154,\n",
              " 0.9978200197219849,\n",
              " 0.9988399744033813]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8PybvELAhv"
      },
      "source": [
        "###Learning Rates:\n",
        "\n",
        "LR = 0.1: Accuracy is 0.9988399744033813 <br>\n",
        "LR = 0.01: Accuracy is 0.999180018901825 <br>\n",
        "LR = 0.001: Accuracy is 0.9965199828147888 <br>\n",
        "\n",
        "LR of 0.01 yields the best accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT3CQBax1-8G"
      },
      "source": [
        "(2a) Now train only the last layer for 1, 0.1, 0.01, and 0.001 while keeping all the other hyperparameters\n",
        "and settings same as earlier for finetuning. Which learning rate gives you the best accuracy on\n",
        "the target dataset ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqP4HU5RS8he"
      },
      "source": [
        "for i in range(176):\n",
        "  model.layers[i].trainable = False\n",
        "\n",
        "LR = [1,0,1,0.01,0.001]\n",
        "for i in LR:\n",
        "  tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9, nesterov=False, name=\"SGD\")\n",
        "\n",
        "  #Compiling Model\n",
        "  model.compile(optimizer='SGD',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "  #Fitting model\n",
        "  history = model.fit(X_train, y_train, batch_size=64, epochs=200, validation_data=(X_test,y_test),shuffle=True,verbose=0)\n",
        "  print(history.history['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHWMg0gg1--v"
      },
      "source": [
        "(2b) For your target dataset find the best final accuracy (across all the learning rates) from the two\n",
        "transfer learning approaches. Which approach and learning rate is the winner? Provide a plausible\n",
        "explanation to support your observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuIy3rCq1_Bh"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DfI3Azq1_Il"
      },
      "source": [
        "1. Both the papers use the same 1B image dataset. However one does weakly supervised pretraining while\n",
        "the other does semi-supervised . What is the difference between weakly supervised and semi-supervised\n",
        "pretraining ? How do they use the same dataset to do two different types of pretraining ? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQYwAunKK8n0"
      },
      "source": [
        "### Solution:\n",
        "Weakly supervised pretraining using noisy labels, whereas semi-supervised learning has a teacher model trained on labeled data to generate labels for the unlabeled data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faZvQP1P1_Lu"
      },
      "source": [
        "2. These questions are based on the paper by Mahajan et al. <br>\n",
        "(a) Are the model trained using hashtags robust against noise in the labels ? What experiments were\n",
        "done in the paper to study this and what was the finding ? Provide numbers from the paper to\n",
        "support your answer. <br>\n",
        "(b) Why is resampling of hashtag distribution important during pretraining for transfer learning ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeOBTVL8K8n0"
      },
      "source": [
        "### Solution\n",
        "\n",
        "Yes, the model trained using hashtags were robust again noise. They pre-trained a ResNeXt model with 1B images and 17k labels where a portion of the labels were randomly replaced with hashtags sampled from the marginal distribution. p = 10% decreased the top-1 accuracy on ImageNet by around 1%, and that p = 25% decreased accuracy by about 2%. <br>\n",
        "\n",
        "Resampling the hashtag distribution allows to make sure that all classes are accounted for, and that there won't be a class left out during training. Since the hashtags followed a Zipfian distribution, if we just sample from that - a lot of classes would be under-represented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PAtYTza1_OJ"
      },
      "source": [
        "3. These questions are based on the paper by Yalzin et al. <br>\n",
        "(a) Why are there two models, a teacher and a student, and how does the student model leverages the\n",
        "teacher model ? Explain why teacher-student modeling is a type of distillation technique. <br>\n",
        "(b) What are the parameters K and P in stage 2 of the approach where unlabeled images are assigned\n",
        "classes using teacher network ? What was the idea behind taking P > 1 ? Explain in your own\n",
        "words. <br>\n",
        "(c) Explain how a new labeled dataset is created using unlabeled images ? Can an image in this new\n",
        "dataset belong to more than one class ? Explain. <br>\n",
        "(d) Refer to Figure 5 in the paper. Why does the accuracy of the student model first improves as we\n",
        "increase the value of K and then decreases ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajldC61HK8n0"
      },
      "source": [
        "### Solution\n",
        "\n",
        "a) The teacher and student model is needed since the teacher model selects the top-K images in the unlabeled dataset and this dataset allows us generate a new training set. Since the student model is always 'smaller' than the teacher model, it is a distillation technique.\n",
        "\n",
        "b) $K$ refers to the number of examples in $\\mathcal{U}$ used to construct $\\hat{\\mathcal{D}}$ for each label in $\\mathcal{D}$. The parameter $P$ determines how many classes a candidate top-K image $I$ is allowed to belong to. The idea behind taking $P > 1$ frequent and non-frequent occurences are both accounted for.\n",
        "\n",
        "c) The new labeled dataset is created from the teacher model that selects the top-K images. For each image in the top-K, the image is then tagged with the classes corresponding to the $P$ largest elements in its softmax vector which allow an image to belong to more than 1 class. \n",
        "\n",
        "d) Increasing $K$ initially lets the student network allowed to be trained on more labeled data, without the noise problem. We will then end up with a higher quality dataset and leads to better performance. But if we keep increasing $K$ past a certain threshold, there will end up being too much noise in the labels which affects the dataset quality which in turn affects performance negatively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xEwMKJE1_Q7"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV1GNyj91_TN"
      },
      "source": [
        "1. Why achieving peak FLOPs from hardware devices like GPUs is a di\u000ecult propostion in real systems\n",
        "? How does PPP help in capturing this ineffciency captured in Paleo model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk8rDjoqK8n0"
      },
      "source": [
        "### Solution:\n",
        "\n",
        "If we were to achieve peak FLOPS, then the framework would need to be optimized for the GPUs which would require in-depth knowledge of the GPUs themselves. PPP measure takes multiple factors into account by displaying the current FLOPs as a percentage of the theoretical maximum afforded by the hardware & software. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gW4pPlM2swX"
      },
      "source": [
        "2. Lu et al. showed that FLOPs consumed by convolution layers in VG16 account for about 99% of the\n",
        "total FLOPS in the forward pass. We will do a similar analysis for VGG19. Calculate FLOPs for\n",
        "di\u000berent layers in VGG19 and then calculate fraction of the total FLOPs attributed by convolution\n",
        "layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMmjt2az2s39"
      },
      "source": [
        "3. Study the tables showing timing benchmarks from Alexnet (Table 2), VGG16 (Table 3), Googlenet\n",
        "(Table 5), and Resnet50 (Table 6). Why the measured time and sum of layerwise timings for forward\n",
        "pass did not match on GPUs ? What approach was adopted in Sec. 5 of the paper to mitigate the\n",
        "measurement overhead in GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPdLoIuTK8n0"
      },
      "source": [
        "### Solution:\n",
        "\n",
        "The measured time and sum of layerwise timing for forward pass did not match on GPUs because CUDA allows for asynchronous computation. The cores on the GPU are sychronized so the timing will be off. The paper resolved this problem by running matrix multiplications asynchronously for many iterations, and with high iterations - the imapct of the overhead is pretty much negligible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAQuZnLt2s9q"
      },
      "source": [
        "4. In Lu et al. FLOPs for di\u000berent layers of a DNN are calculated. Use FLOPs numbers for VGG16\n",
        "(Table 3), Googlenet (Table 5), and Resnet50 (Table 6), and calculate the inference time (time to have\n",
        "a forward pass with one image) using published T\n",
        "ops number for K80 (Refer to NVIDIA TESLA\n",
        "GPU Accelerators). Use this to calculate the peak (theoretical) throughput achieved with K80 for these\n",
        "3 models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJ1X5Nnr3XS7"
      },
      "source": [
        "### Solution\n",
        "\n",
        "Tesla K80: double precision peak performance of 1.87 Tflops. \n",
        "\n",
        "Forward pass on VGG requires 15503M FLOPs; as such, one forward pass on a K80 would take $(15503 \\times 10^6) / (1.87 \\times 10^{12}) = 0.00829037433s$ so throughput is 120 images/sec.\n",
        "\n",
        "GoogLeNet inference time: $(1606 * 10^6) / (1.87 \\times 10^{12}) = 0.00085882352s$ per image. Throughoutput: 1164 images/sec.\n",
        "\n",
        "ResNet inference time: $(3922 * 10^6) / (1.87 \\times 10^{12}) = 0.0020973262s$. Throughput: 477 images/sec."
      ]
    }
  ]
}